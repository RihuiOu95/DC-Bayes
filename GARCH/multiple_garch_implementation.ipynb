{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16437aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling libraries:\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np, time, matplotlib.pyplot as plt, math, pandas, numpy.random as npr, pystan as ps, pickle\n",
    "from pylab import plot, show, legend\n",
    "from functions_garch import *\n",
    "import arviz as az\n",
    "import random\n",
    "import os\n",
    "import pathlib\n",
    "from time import time\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78d67dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "p = 5\n",
    "q = 3\n",
    "d = 3\n",
    "X = npr.randn(d,T)\n",
    "omega = 1.0\n",
    "b = npr.randn(d)\n",
    "\n",
    "def generate_Y(T, omega, rescale, b, X):\n",
    "    beta = np.ones(p) / (rescale * p)\n",
    "    alpha = np.ones(q) / (rescale * q)\n",
    "    sigsq, y = simulate_data(T, omega, beta, alpha, b, X)\n",
    "    return sigsq, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa06aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0: create multiple datasets \n",
    "rescales = [2.00, 3.00, 5.00]\n",
    "datas = {}\n",
    "n_exp = len(rescales)\n",
    "for i in range(n_exp):\n",
    "    rescale = rescales[i]\n",
    "    datas[rescale] = {}\n",
    "    sigsq, y = generate_Y(T, omega, rescale, b, X)\n",
    "    datas[rescale]['alpha'] = np.ones(q) / (rescale * q)\n",
    "    datas[rescale]['beta'] = np.ones(p) / (rescale * p)\n",
    "    datas[rescale]['omega'] = omega\n",
    "    datas[rescale]['b'] = b\n",
    "    datas[rescale]['sigsq'], datas[rescale]['y'] = sigsq, y\n",
    "    datas[rescale]['X'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6988cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: make path and save data\n",
    "pathlib.Path('multiple_sims').mkdir(parents=True, exist_ok=True)\n",
    "file = open(\"multiple_sims/datas.pkl\", \"wb\")\n",
    "pickle.dump(datas, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22868557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_f0ef85016fbe1114f4bfb9978c846a08 NOW.\n",
      "In file included from /var/folders/vm/bnpn0t152gn54b0_nhp78xf40000gp/T/pystan_5wvqf7so/stanfit4anon_model_f0ef85016fbe1114f4bfb9978c846a08_3167031423563654957.cpp:771:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1944:\n",
      "/opt/anaconda3/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: \"Using deprecated NumPy API, disable it with \"          \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "#warning \"Using deprecated NumPy API, disable it with \" \\\n",
      " ^\n",
      "/var/folders/vm/bnpn0t152gn54b0_nhp78xf40000gp/T/pystan_5wvqf7so/stanfit4anon_model_f0ef85016fbe1114f4bfb9978c846a08_3167031423563654957.cpp:9546:30: warning: comparison of integers of different signs: 'Py_ssize_t' (aka 'long') and 'std::__1::vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >::size_type' (aka 'unsigned long') [-Wsign-compare]\n",
      "    __pyx_t_12 = ((__pyx_t_9 != __pyx_v_fitptr->param_names_oi().size()) != 0);\n",
      "                   ~~~~~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "In file included from /var/folders/vm/bnpn0t152gn54b0_nhp78xf40000gp/T/pystan_5wvqf7so/stanfit4anon_model_f0ef85016fbe1114f4bfb9978c846a08_3167031423563654957.cpp:780:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/pystan/py_var_context.hpp:12:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/pystan/stan/src/stan/io/dump.hpp:6:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/pystan/stan/lib/stan_math/stan/math/prim/mat.hpp:336:\n",
      "In file included from /opt/anaconda3/lib/python3.9/site-packages/pystan/stan/lib/stan_math/stan/math/prim/mat/prob/poisson_log_glm_log.hpp:5:\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pystan/stan/lib/stan_math/stan/math/prim/mat/prob/poisson_log_glm_lpmf.hpp:64:59: warning: unused typedef 'T_alpha_val' [-Wunused-local-typedef]\n",
      "      typename partials_return_type<T_alpha>::type>::type T_alpha_val;\n",
      "                                                          ^\n",
      "In file included from /var/folders/vm/bnpn0t152gn54b0_nhp78xf40000gp/T/pystan_5wvqf7so/stanfit4anon_model_f0ef85016fbe1114f4bfb9978c846a08_3167031423563654957.cpp:783:\n",
      "/var/folders/vm/bnpn0t152gn54b0_nhp78xf40000gp/T/pystan_5wvqf7so/anon_model_f0ef85016fbe1114f4bfb9978c846a08.hpp:179:24: warning: unused typedef 'local_scalar_t__' [-Wunused-local-typedef]\n",
      "        typedef double local_scalar_t__;\n",
      "                       ^\n",
      "4 warnings generated.\n"
     ]
    }
   ],
   "source": [
    "# step 2: do multiple simulations\n",
    "sm = ps.StanModel(file=\"garch_WB.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5053da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.a: def simulation functions\n",
    "n_chains = 1\n",
    "n_iter = 1000\n",
    "\n",
    "def full_mcmc(rescale, n_iter=500):\n",
    "    \n",
    "    # extract data\n",
    "    print(\"Doing Full MCMC for rescale=\"+str(rescale))\n",
    "    y = datas[rescale]['y']\n",
    "    X = datas[rescale]['X'] \n",
    "    data = dict(T=T, p=p, q=q, r=max(p,q), d=d, y=y, X=X, power=1.)\n",
    "    \n",
    "    # MCMC\n",
    "    start = time()\n",
    "    fit = sm.sampling(data=data, thin=1, n_jobs=8, chains=n_chains, init=\"random\", iter=n_iter)\n",
    "    mle = sm.optimizing(data=data)\n",
    "    print(round((time()-start)/60,2), \"minutes to run\")\n",
    "    \n",
    "    # make path\n",
    "    rescale_str = str(rescale)\n",
    "    path = 'multiple_sims/'+ rescale_str\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # save\n",
    "    print(round((time()-start)/60,2), \"minutes to run\")\n",
    "    trace = fit.extract()\n",
    "    file = open(path+\"/full_mcmc.pkl\", \"wb\")\n",
    "    pickle.dump(trace, file)\n",
    "    file.close()\n",
    "    file = open(path+\"/full_mle.pkl\", \"wb\")\n",
    "    pickle.dump(mle, file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1782600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_mcmc(rescale,n_iter=500,m=10):\n",
    "    m = 10\n",
    "    rescale_str = str(rescale)\n",
    "    tstarts = np.arange(m).astype(int)\n",
    "    tends = 1 + tstarts\n",
    "    tstarts *= int(T/m)\n",
    "    tends *= int(T/m)\n",
    "    print(\"Doing DC MCMC for alpha=\"+rescale_str)\n",
    "    for i in range(m) :\n",
    "        tstart, tend = tstarts[i], tends[i]\n",
    "        \n",
    "        # extract data\n",
    "        y = datas[rescale]['y']\n",
    "        X = datas[rescale]['X'] \n",
    "        data = dict(T=tend-tstart, p=p, q=q, r=max(p,q), d=d, y=y[tstart:tend], X=X[:,tstart:tend], power=T/(tend-tstart))\n",
    "        \n",
    "        # MCMC\n",
    "        fit = sm.sampling(data=data, thin=1, n_jobs=8, chains=n_chains, init=\"random\", iter=n_iter)\n",
    "        trace = fit.extract()\n",
    "        \n",
    "        # make path\n",
    "        path = 'multiple_sims/'+rescale_str\n",
    "        pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # save files\n",
    "        file = open(path+\"/mcmc_wb_chunk\"+str(i+1)+\".pkl\", \"wb\")\n",
    "        pickle.dump(trace, file)\n",
    "        file.close()\n",
    "        \n",
    "        mle = sm.optimizing(data=data)\n",
    "        file = open(path+\"/mle_chunk\"+str(i+1)+\".pkl\", \"wb\")\n",
    "        pickle.dump(mle, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0974cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Full MCMC for rescale=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:11 of 500 iterations ended with a divergence (2.2 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:6 of 500 iterations ended with a divergence (1.2 %).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting ini8.06 tial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "\n",
      "Gradient evaluation took 0.004678 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 46.78 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iterationminutes to run\n",
      "8.06 minutes to run\n",
      ": 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 264.734 seconds (Warm-up)\n",
      "               221.819 seconds (Sampling)\n",
      "               486.553 seconds (Total)\n",
      "\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Initial log joint probability = -197191\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      13      -5931.75   0.000398674     0.0341323           1           1       15   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "Doing DC MCMC for alpha=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000401 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.01 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 13.388 seconds (Warm-up)\n",
      "               11.9562 seconds (Sampling)\n",
      "               25.3442 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -86509\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -5496.42      0.457103       37.2148      0.5496           1       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -5493.73     0.0984448      0.488382           1           1       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -5493.65       2.59958       4.80922           1           1       67   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -5493.08     0.0345769       1.73991      0.5945      0.5945       92   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -5492.92      0.291988      0.662266           1           1      113   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -5492.86      0.166933      0.490135      0.6361      0.6361      133   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -5492.85       0.17311     0.0943635           1           1      157   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -5492.84      0.122569      0.552453           1           1      177   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     161      -5492.84     0.0225936     0.0652652           1           1      179   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000389 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.89 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 8.39493 seconds (Warm-up)\n",
      "               6.62388 seconds (Sampling)\n",
      "               15.0188 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -175633\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -6111.02      0.206861        20.505      0.6776      0.6776       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39       -6088.7      0.147717       9.34953           1           1       52   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -6086.07      0.177299       4.41978      0.8353      0.8353       73   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -6085.89     0.0899128      0.837961           1           1       94   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99       -6085.8     0.0253208      0.555978      0.1923      0.1923      117   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -6085.79     0.0636544      0.101341           1           1      141   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -6085.79      0.063617     0.0509114           1           1      166   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     144      -6085.79     0.0147301     0.0492927      0.6776      0.6776      172   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000404 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.04 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 27.3834 seconds (Warm-up)\n",
      "               28.4625 seconds (Sampling)\n",
      "               55.8459 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -162531\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -6777.15      0.186118        8.7213           1           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -6775.87     0.0308502      0.727274           1           1       45   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -6775.84     0.0561031        1.4504           1           1       69   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -6775.69      0.251304       1.48998      0.9606      0.9606       91   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -6775.63     0.0014507      0.067354           1           1      111   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:4 of 500 iterations ended with a divergence (0.8 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gradient evaluation took 0.000396 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.96 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 11.6572 seconds (Warm-up)\n",
      "               10.3188 seconds (Sampling)\n",
      "               21.976 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -61730.5\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -5950.39     0.0981292       6.75294           1           1       21   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -5949.46    0.00644327      0.263485           1           1       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -5944.32      0.320676       10.9248           1           1       76   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -5941.79     0.0240808       1.58972      0.5558      0.5558      103   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -5941.55      0.117963      0.881157           1           1      123   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119       -5941.5     0.0218092      0.538866           1           1      147   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:5 of 500 iterations ended with a divergence (1 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -5941.45      0.191159      0.592213           1           1      168   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -5941.44     0.0352515      0.063629           1           1      190   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -5941.44      0.689368      0.249474           1           1      212   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -5941.43       4.62158      0.328352           1           1      233   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     204      -5941.43     0.0870301     0.0664862           1           1      238   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000405 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.05 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 13.0946 seconds (Warm-up)\n",
      "               10.9954 seconds (Sampling)\n",
      "               24.09 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -68280.8\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -6277.27      0.241481       12.4568      0.5908           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -6275.77       0.60857      0.331518      0.7804      0.7804       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      53      -6275.76      0.087782     0.0274149           1           1       59   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000816 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 8.16 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 17.4365 seconds (Warm-up)\n",
      "               19.4266 seconds (Sampling)\n",
      "               36.8631 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -179796\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:23 of 500 iterations ended with a divergence (4.6 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      19      -5236.35      0.327011       27.3384           1           1       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39       -5228.6      0.164695       3.53783           1           1       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -5228.44     0.0411805      0.774811           1           1       64   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -5228.33      0.243167      0.872028           1           1       87   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -5228.27    0.00401769      0.145762      0.5269      0.5269      111   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     108      -5228.27    0.00436302     0.0635012      0.5033           1      122   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000382 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.82 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 23.8259 seconds (Warm-up)\n",
      "               13.7137 seconds (Sampling)\n",
      "               37.5395 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -74596\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:3 of 500 iterations ended with a divergence (0.6 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -5130.28     0.0821605       6.32065           1           1       20   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -5119.51         0.261       22.3217      0.7629      0.7629       43   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -5108.56      0.592372       34.1132           1           1       69   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -5108.21     0.0863457       3.06401           1           1       94   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -5107.82     0.0598384       4.72531      0.4325      0.4325      117   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -5107.71      0.011936       1.43302           1           1      138   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139       -5107.7      0.101697      0.297706      0.1951           1      161   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     150       -5107.7     0.0872398      0.069617           1           1      172   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000403 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.03 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 15.6147 seconds (Warm-up)\n",
      "               15.2208 seconds (Sampling)\n",
      "               30.8355 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -58807.7\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -5768.06      0.554587       8.37823           1           1       21   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -5767.42     0.0206767       2.73154      0.6075      0.6075       42   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -5766.48     0.0468268      0.736997      0.4017           1       66   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      66      -5766.48    0.00315177     0.0335544           1           1       73   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000388 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.88 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 11.8723 seconds (Warm-up)\n",
      "               10.1917 seconds (Sampling)\n",
      "               22.064 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -47027.3\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -6878.64     0.0686525       11.5529           1           1       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -6875.88    0.00518851       2.51357      0.2042      0.2042       48   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -6875.38     0.0301673        4.1131      0.9764     0.09764       72   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -6871.03     0.0292294       7.45927           1           1       97   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -6870.21      0.301293       5.02737           1           1      118   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -6869.68     0.0549687       2.65644      0.1401           1      141   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:25 of 500 iterations ended with a divergence (5 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:7 of 500 iterations ended with a divergence (1.4 %).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -6869.58       1.10264       1.50703           1           1      163   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -6869.56     0.0401285      0.328011      0.5599      0.5599      186   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     168      -6869.56      0.304621     0.0591424           1           1      198   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000404 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.04 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 56.5137 seconds (Warm-up)\n",
      "               52.7927 seconds (Sampling)\n",
      "               109.306 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -69304.8\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -10132.9      0.381307       63.5587      0.3849           1       28   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -10127.5     0.0429069       2.15183      0.1725           1       50   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -10127.3     0.0173923       1.00925       0.579       0.579       76   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -10127.2      0.154323       8.37107      0.1502           1       99   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99        -10127     0.0859909       5.22405      0.1743     0.01743      125   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -10121.9      0.190161       11.6728           1           1      152   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -10117.7      0.230447       5.72893      0.7015      0.7015      174   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -10117.4    0.00393927       0.42681           1           1      197   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -10117.4      0.206776       3.33513           1           1      218   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -10117.3     0.0670078      0.339557      0.6028      0.6028      240   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     207      -10117.3     0.0112865      0.118578      0.1717           1   Doing Full MCMC for rescale=3.0\n",
      "   249   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probabi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lity evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluate7.2 minutes to run\n",
      "s to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), 7.2 minutes to run\n",
      "Doing DC MCMC for alpha=3.0i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative\n",
      " infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "\n",
      "Gradient evaluation took 0.003893 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 38.93 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 229.196 seconds (Warm-up)\n",
      "               204.074 seconds (Sampling)\n",
      "               433.27 seconds (Total)\n",
      "\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Initial log joint probability = -320400\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3826.62    0.00852737       1.71193      0.2551           1       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      31      -3826.61   0.000519402     0.0355826           1           1       35   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000848 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 8.48 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 10.0766 seconds (Warm-up)\n",
      "               10.4474 seconds (Sampling)\n",
      "               20.524 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -72998.4\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3554.35      0.514933       26.1374      0.5013           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39       -3549.2     0.0164011       2.45865           1           1       43   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59         -3549     0.0995383       1.70404           1           1       65   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3548.53       1.38321       2.33506           1           1       88   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3548.47      0.445281      0.314719           1           1      108   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3548.47     0.0333593     0.0676074           1           1      131   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:7 of 500 iterations ended with a divergence (1.4 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     123      -3548.47     0.0065828     0.0338758           1           1      136   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000379 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.79 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 15.0071 seconds (Warm-up)\n",
      "               11.7531 seconds (Sampling)\n",
      "               26.7602 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -38192\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3483.94      0.113875       2.78732           1           1       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3475.94      0.371754       30.9546      0.6069           1       52   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3472.98     0.0619495       8.30625           1           1       78   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3471.66      0.324363        12.655      0.4554           1      101   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3471.27      0.595893      0.884085           1           1      123   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3471.22     0.0189465       0.30668           1           1      145   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     126      -3471.22    0.00104976     0.0120931           1           1      153   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000389 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.89 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 13.653 seconds (Warm-up)\n",
      "               12.8772 seconds (Sampling)\n",
      "               26.5303 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -57676.4\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -4007.69     0.0758102       4.46191           1           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -4002.41      0.163689       5.02149           1           1       48   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -4002.24    0.00180457      0.228748           1           1       70   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79       -4001.5     0.0652037       2.02787      0.9064      0.9064       94   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -4001.35    0.00360595       1.37794      0.5561      0.5561      114   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -4001.21     0.0327814      0.718864           1           1      136   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -4001.13      0.039477       0.43283           1           1      156   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -4001.11    0.00656698      0.266949           1           1      176   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179       -4001.1    0.00169909     0.0853368           1           1      199   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199       -4001.1     0.0200848      0.648945      0.1712           1      221   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     219      -4001.09     0.0631965      0.950162           1           1      242   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     239      -4001.09      0.301752       0.92426           1           1      263   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     259      -4001.08      0.229268      0.271659           1           1      284   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     279      -4001.07      0.102599      0.143857           1           1      305   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     299      -4001.07      0.148669      0.241903           1           1      327   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     303      -4001.07      0.259915     0.0378605           1           1      332   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000397 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.97 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 23.9485 seconds (Warm-up)\n",
      "               17.7778 seconds (Sampling)\n",
      "               41.7263 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -51032.5\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19       -3728.4      0.349905       137.294       0.421           1       25   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3704.02     0.0157037     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:7 of 500 iterations ended with a divergence (1.4 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4.1496           1           1       47   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3703.68       0.14928       5.61264      0.4105           1       68   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3702.99    0.00478705      0.973027       0.474       0.474       90   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3702.97    0.00404335      0.424147          10           1      113   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3702.95    0.00219709      0.283432      0.3972      0.8239      138   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3702.91     0.0140882      0.777629      0.4118           1      162   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3702.86      0.117879      0.808156      0.7424      0.7424      184   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3702.86    0.00459473      0.679757           1           1      205   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     196      -3702.85   0.000246591     0.0818017      0.7134      0.7134      224   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000387 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.87 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 19.1404 seconds (Warm-up)\n",
      "               13.7658 seconds (Sampling)\n",
      "               32.9062 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -168051\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3606.15      0.409378       45.4014           1           1       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3596.79     0.0481654       32.5591      0.1344      0.1344       48   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3592.15      0.659363       18.9525           1           1       70   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3591.44      0.102832       3.58954           1           1       93   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3589.11     0.0254348       8.89954      0.5133      0.5133      115   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3587.92      0.118668        6.2486       0.953       0.953      136   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3586.66     0.0735394       6.64904           1           1      158   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3586.34      0.079943      0.444512      0.9945      0.9945      181   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179       -3586.3     0.0121601      0.832401           1           1      205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:1 of 500 iterations ended with a divergence (0.2 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -3586.11     0.0972985      0.610235           1           1      229   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     219      -3586.08       4.95928       2.78976           1           1      251   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     239      -3586.08     0.0646071      0.742964      0.5635      0.5635      272   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     259      -3586.02       1.04181      0.637728           1           1      294   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     279      -3586.02      0.160711      0.444443           1           1      318   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     299      -3585.99     0.0125318       0.30205           1           1      339   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     309      -3585.99    0.00341198     0.0506747           1           1      350   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000385 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.85 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 16.7585 seconds (Warm-up)\n",
      "               17.5932 seconds (Sampling)\n",
      "               34.3517 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -169909\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3775.28     0.0113506       6.26922      0.2617      0.2617       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39       -3773.7     0.0335454        4.9361      0.5248      0.5248       45   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3773.34     0.0713152       2.92257      0.4041           1       67   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3773.24     0.0361693       1.58182           1           1       89   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3773.18    0.00684924     0.0813167           1           1      111   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3773.17     0.0908567      0.565812           1           1      134   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3773.16       1.05392      0.909454           1           1      156   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3773.15     0.0778945      0.152268           1           1      177   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     177      -3773.15     0.0307166      0.023797      0.6453      0.6453      196   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:6 of 500 iterations ended with a divergence (1.2 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000378 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.78 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 13.9915 seconds (Warm-up)\n",
      "               7.72611 seconds (Sampling)\n",
      "               21.7177 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -138854\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -4051.33      0.492549       40.5865           1           1       24   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -4048.78      0.519841       2.32247       0.949       0.949       45   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -4048.75        4.1636       1.22648         2.6           1       70   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "Error evaluating model log probability: Non-finite gradient.\n",
      "\n",
      "      79      -4047.11      0.138222       3.87727           1           1      101   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -4047.09    0.00647135      0.628287      0.6718      0.6718      122   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -4047.08    0.00705484       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:19 of 500 iterations ended with a divergence (3.8 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38426           1           1      145   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -4047.07    0.00792288      0.281702           1           1      166   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -4047.07    0.00288942      0.433574       0.119           1      189   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -4047.06     0.0246764      0.469032           1           1      211   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -4047.04    0.00784635      0.188023           1           1      234   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     219      -4047.04    0.00612329      0.133413      0.2066           1      257   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     223      -4047.04    0.00838635     0.0412053           1           1      261   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000382 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.82 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 26.1437 seconds (Warm-up)\n",
      "               17.9922 seconds (Sampling)\n",
      "               44.1359 seconds (Total)\n",
      "\n",
      "Initial"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:31 of 500 iterations ended with a divergence (6.2 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:1 of 500 iterations saturated the maximum tree depth of 10 (0.2 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " log joint probability = -54271.7\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -4105.07      0.189031       25.6508       0.717       0.717       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -4097.19     0.0219968       2.30013      0.1869           1       46   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -4096.27     0.0260739      0.659449           1           1       68   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -4096.23      0.182429      0.478442           1           1       88   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -4096.21     0.0137133      0.247849       0.814       0.814      112   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:14 of 500 iterations ended with a divergence (2.8 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:18 of 500 iterations ended with a divergence (3.6 %).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha0  # evals  Notes \n",
      "     113      -4096.21     0.0594145     0.0536344           1           1      128   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000396 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.96 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 84.0102 seconds (Warm-up)\n",
      "               80.0464 seconds (Sampling)\n",
      "               164.057 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -184923\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3996.33      0.151183       17.0049           1           1       25   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3983.19       0.41871       51.1183           1           1       54   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3980.55    0.00853848       13.3184           1           1       78   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3980.07      0.118792       73.2545      0.6083           1      101   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3979.53     0.0996253       42.0411           1           1      129   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3979.29      0.100986       32.5431           1           1      155   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3978.98      0.122762       63.0244      0.6213           1      180   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3977.92      0.298496       47.3316           1           1      207   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3976.95      0.537734       77.4099      0.2156           1      233   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -3975.71      0.818483       57.7317      0.9447      0.9447      260   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     219      -3975.01    0.00245651       11.1851      0.7105      0.7105      285   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     239      -3974.71     0.0646924       23.0351      0.1915           1      310   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     259      -3973.38      0.279155       17.8299      0.4476      0.4476      336   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     279      -3973.21      0.688793       7.65947           1           1      363   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     299       -3973.2     0.0188749      0.955643           1           1      385   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     319      -3973.11      0.173255       13.9752           1           1      411   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     339      -3972.9Doing Full MCMC for rescale=5.0\n",
      "9      0.614096       3.87109       0.213           1      433   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     359       -3972.8       8.48838       12.1624           1           1      459   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     379      -3972.74       2.20214       2.27605      0.3297           1      493   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     399      -3972.71      0.200983       1.66825           1           1      518   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     415      -3972.71      0.187754      0.431886           1           1      538   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000414 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 11.6473 seconds (Warm-up)\n",
      "               6.62683 seconds (Sampling)\n",
      "               18.2741 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -69457\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19       -3766.7      0.105146       9.52359           1           1       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3763.39     0.08385.99 minutes to run\n",
      "5.99 minutes to run\n",
      "Doing DC MCMC for alpha=5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633       2.79265           1           1       45   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3762.65     0.0469877       1.19197           1           1       66   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3762.58      0.134325      0.530215           1           1       87   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3762.54      0.495871      0.349342      0.9491      0.9491      107   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3762.53       3.70137      0.152649      0.8673      0.8673      128   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     125      -3762.53       3.27848     0.0528149           1           1      135   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "\n",
      "Gradient evaluation took 0.004203 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 42.03 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteratio"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:6 of 500 iterations ended with a divergence (1.2 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 216.746 seconds (Warm-up)\n",
      "               143.861 seconds (Sampling)\n",
      "               360.607 seconds (Total)\n",
      "\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Rejecting initial value:\n",
      "  Log probability evaluates to log(0), i.e. negative infinity.\n",
      "  Stan can't start sampling from this initial value.\n",
      "Initial log joint probability = -347519\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3346.21    0.00806993       3.47014      0.3364      0.3364       27   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      31      -3346.17   0.000314283     0.0334698      0.1459      0.9895       41   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000441 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.41 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 89.5673 seconds (Warm-up)\n",
      "               5.73733 seconds (Sampling)\n",
      "               95.3047 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -103063\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3259.37       2.65862       12.0108           1           1       25   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3257.54       10.6067       13.7451      0.5936      0.5936       50   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      54      -3256.91     0.0448918     0.0806878      0.9273      0.9273       68   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000427 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.27 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 14.8688 seconds (Warm-up)\n",
      "               9.34615 seconds (Sampling)\n",
      "               24.215 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -104751\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3587.03       0.22873       15.6837           1           1       23   \n",
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:40 of 500 iterations ended with a divergence (8 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3585.42      0.421669       20.4549      0.6284      0.6284       46   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3584.44     0.0131838      0.956394           1           1       70   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3584.43    0.00153313      0.358378      0.5057      0.5057       94   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3584.19     0.0315674       9.49864      0.1603      0.1603      120   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3582.58     0.0424829       14.5029      0.5373      0.5373      146   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139       -3582.1     0.0556331       2.18873           1           1      169   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3581.96      0.246609       1.76448           1           1      190   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3581.93    0.00892863      0.630832      0.3066      0.3066      214   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -3581.92     0.0545764       1.50036      0.1842           1      236   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     219       -3581.9     0.0987621       1.05622           1           1      259   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     239      -3581.88      0.458074      0.912479      0.4606           1      282   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     259      -3581.87      0.040489      0.798059      0.4655      0.4655      304   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     276      -3581.86     0.0430983     0.0588543           1           1      322   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000403 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.03 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 22.2363 seconds (Warm-up)\n",
      "               10.7112 seconds (Sampling)\n",
      "               32.9475 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -156194\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3206.57      0.139904       3.81316           1           1       27   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3205.23     0.0209135       2.07003           1           1       50   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3202.59      0.330492       5.10494      0.3684      0.6991       79   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79       -3202.4     0.0108926      0.479139      0.7533      0.7533      100   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3202.37    0.00426499      0.428071           1           1      120   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3202.22     0.0306371      0.436229       0.901       0.901      141   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139       -3202.2    0.00417861      0.314269           1           1      162   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159       -3202.2     0.0208379      0.217167           1           1      183   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     172       -3202.2    0.00128569     0.0687071           1           1      197   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:1 of 500 iterations ended with a divergence (0.2 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000385 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.85 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 17.778 seconds (Warm-up)\n",
      "               6.08989 seconds (Sampling)\n",
      "               23.8679 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -117923\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3224.23      0.246425       7.27532           1           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3212.69      0.808882        32.391      0.6754           1       47   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59       -3194.3      0.253231       54.3035      0.4927       0.912       74   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3191.76     0.0276371       12.2485      0.7746      0.7746       96   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3191.47     0.0322994       9.60552           1           1      117   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119       -3190.3      0.686669       13.9026      0.3174           1      139   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     136      -3190.22      0.233362      0.152033           1           1      158   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:3 of 500 iterations ended with a divergence (0.6 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000401 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.01 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 14.4131 seconds (Warm-up)\n",
      "               10.0627 seconds (Sampling)\n",
      "               24.4758 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -101683\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3402.95     0.0475801       16.3694      0.6177      0.6177       20   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3401.96     0.0400142        3.4778           1           1       40   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3401.84     0.0145439       1.97671      0.7818      0.7818       63   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3400.08      0.109484       5.56324           1           1       87   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3399.31    0.00806648       1.16405           1           1      109   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3399.25     0.0573117       2.96753           1           1      130   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139       -3399.1     0.0821752       9.75473     0.09868           1      151   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3398.34      0.510612       4.37652      0.4437      0.6892      178   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3398.14     0.0104858      0.483404       0.622       0.622      199   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199      -3398.14     0.0158093      0.206384           1           1      221   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     205      -3398.14    0.00155148     0.0202074       1.353      0.1353      229   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:9 of 500 iterations ended with a divergence (1.8 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000379 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.79 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 15.4844 seconds (Warm-up)\n",
      "               11.0386 seconds (Sampling)\n",
      "               26.523 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -95800.1\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3481.45     0.0820035       11.1639           1           1       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3477.45      0.178572       11.0984           1           1       43   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3476.27      0.471174       3.03841           1           1       64   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3476.13       0.28864      0.878094           1           1       87   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3476.11      0.270161       0.68324           1           1      109   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3476.09      0.305096       1.38786           1           1      130   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3476.06      0.032837      0.433892      0.3779      0.3779      154   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3475.82       0.43517       1.26354      0.6235      0.6235      182   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3475.81     0.0569065     0.0632781           1           1      207   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     180      -3475.81     0.0118631      0.095179      0.9428      0.9428      208   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:24 of 500 iterations ended with a divergence (4.8 %).\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient evaluation took 0.000395 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.95 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 25.5807 seconds (Warm-up)\n",
      "               19.299 seconds (Sampling)\n",
      "               44.8797 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -195249\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3412.47      0.154438       9.16731      0.7808      0.7808       22   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3408.39       1.32884       22.2264           1           1       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3407.53       0.16295       12.4955           1           1       70   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3405.36       0.25438       4.97326      0.9836      0.9836       93   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3404.48      0.143418      0.734021           1           1      117   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:40 of 500 iterations ended with a divergence (8 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -3404.24     0.0491725       2.02568      0.6618      0.6618      139   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3404.06      0.713567       1.45334           1           1      163   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3404.01       2.56138       1.31679      0.3649           1      186   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     169         -3404       0.24102    0.00678978           1           1      196   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000395 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.95 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 15.2619 seconds (Warm-up)\n",
      "               6.51185 seconds (Sampling)\n",
      "               21.7737 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -169635\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3151.36      0.331983       6.55841           1           1       23   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3147.36      0.372957       31.9282           1           1       44   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59         -3146      0.746754       5.99087           1           1       69   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79       -3145.6      0.346844       6.23912           1           1       89   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3145.48    0.00346447       1.11281           1           1      110   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3145.44    0.00674023       1.08397      0.3443           1      133   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:32 of 500 iterations ended with a divergence (6.4 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3145.39     0.0473565       1.96274           1           1      153   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3145.28       2.61474       3.78025           1           1      174   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3145.27       3.99316      0.947709           1           1      195   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     187      -3145.27     0.0794546     0.0902342       0.464       0.464      204   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.00039 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 3.9 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 16.5953 seconds (Warm-up)\n",
      "               13.0419 seconds (Sampling)\n",
      "               29.6372 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -153452\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19         -3328      0.161683       12.4171           1           1       25   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3326.72      0.552421       12.9568       5.136           1       49   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3325.64     0.0439119       5.19608      0.6184      0.6184       73   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3321.99       1.41271       36.7839           1           1       96   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3319.94      0.279361       1.60462           1           1      117   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119      -3319.86      0.115735      0.450062      0.8279      0.8279      140   \n",
      "    Iter      log "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "Exception: normal_lpdf: Scale parameter is 0, but must be > 0!  (in 'garch_WB.stan' at line 39)\n",
      "\n",
      "If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "\n",
      "WARNING:pystan:4 of 500 iterations ended with a divergence (0.8 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     128      -3319.86     0.0112042     0.0700934      0.6827      0.6827      149   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "\n",
      "Gradient evaluation took 0.000409 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 4.09 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Iteration:   1 / 1000 [  0%]  (Warmup)\n",
      "Iteration: 100 / 1000 [ 10%]  (Warmup)\n",
      "Iteration: 200 / 1000 [ 20%]  (Warmup)\n",
      "Iteration: 300 / 1000 [ 30%]  (Warmup)\n",
      "Iteration: 400 / 1000 [ 40%]  (Warmup)\n",
      "Iteration: 500 / 1000 [ 50%]  (Warmup)\n",
      "Iteration: 501 / 1000 [ 50%]  (Sampling)\n",
      "Iteration: 600 / 1000 [ 60%]  (Sampling)\n",
      "Iteration: 700 / 1000 [ 70%]  (Sampling)\n",
      "Iteration: 800 / 1000 [ 80%]  (Sampling)\n",
      "Iteration: 900 / 1000 [ 90%]  (Sampling)\n",
      "Iteration: 1000 / 1000 [100%]  (Sampling)\n",
      "\n",
      " Elapsed Time: 11.3421 seconds (Warm-up)\n",
      "               6.01469 seconds (Sampling)\n",
      "               17.3568 seconds (Total)\n",
      "\n",
      "Initial log joint probability = -167529\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      19      -3263.77        0.1554       19.6227           1           1       26   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      39      -3258.91     0.0735087       5.84238           1           1       49   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      59      -3256.95      0.193007       9.11463           1           1       71   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      79      -3255.48      0.505793       12.7243           1           1       92   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99      -3255.41      0.020349       1.18339           1           1      112   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     119       -3255.4    0.00242987      0.525752      0.6412      0.6412      133   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     139      -3255.37      0.242534        1.2132           1           1      157   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     159      -3255.36     0.0411894       0.59148           1           1      178   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     179      -3255.35    0.00133968      0.129395      0.2216      0.2216      199   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     181      -3255.35    0.00280991     0.0610974      0.4679           1      202   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    }
   ],
   "source": [
    "# step 3: run multi sims\n",
    "n_iter = 1000\n",
    "for rescale in rescales:\n",
    "    full_mcmc(rescale, n_iter)\n",
    "    dc_mcmc(rescale, n_iter, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dec2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
