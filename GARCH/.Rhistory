kappa(data)
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=300, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=300, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
duplicated(t(data))
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
duplicated(t(data))
sigma = cor(data)
for (i in 1:42){
for (j in 1:42){
if (sigma[i, j] > 0.9 && (i!=j)){
print(c(i, j))
}
}
}
library(dplyr)
generate_data<-function(n, B, p, intercept, binom_prob, rnorm_mean, rnorm_sd, ep_sd, strong, zero, delta){
inter    <- intercept
strong_n <- p * strong #2
if (p %% 2 != 0){
norm_n  <- (p+1)/2 #3
binom_n <- p - norm_n #2
} else {
norm_n  <- p/2 #10
binom_n <- p/2 #10
}
# distribute binary/continuous for strong beta
if (strong_n %% 2 == 0){
strong_n_con  <- strong_n/2
strong_n_bin  <- strong_n/2
} else {
strong_n_con  <- (strong_n + 1)/2
strong_n_bin <- strong_n - strong_n_con
}
# distribute binary/continuous for zero beta
if (zero %% 2 == 0){
zero_n_con  <- zero/2
zero_n_bin  <- zero/2
} else {
zero_n_con  <- (zero_n + 1)/2
zero_n_bin <- zero_n - zero_n_con
}
# distribute binary/continuous
weak_n_con <- norm_n - strong_n_con - zero_n_con
weak_n_bin <- binom_n - strong_n_bin - zero_n_bin
if (weak_n_con <= 0){
weak_n_con = 0
}
if (weak_n_bin <= 0){
weak_n_bin = 0
}
# beta
binary_beta     <- c(rep(1, strong_n_bin), rep(0.5, weak_n_bin), rep(0, zero_n_bin))
con_beta        <- c(rep(1, strong_n_con), rep(0.5, weak_n_con), rep(0, zero_n_con))
true_beta       <- c(binary_beta, con_beta)
true_beta_table <- data.frame(variable = paste0("XZ", 1:p), true_beta)
# y prediction with interaction
x_all_var <- matrix(0, nrow=n, ncol=p)
z         <- rbinom(n, size=1, 0.5) # treatment indicator
for (j in 1:binom_n) {
x_all_var[, j] <- rbinom(n, size = 1, prob = binom_prob)
}
for (k in (norm_n+1):p) {
x_all_var[, k] <- rnorm(n, mean = rnorm_mean, sd = rnorm_sd)
}
gamma <- rep(1,p)
del_mat <- rep(delta,p)
interaction_term <- z*x_all_var
colnames(interaction_term) <- paste0('XZ',1:p)
linpred <- inter + x_all_var%*%gamma + interaction_term %*% true_beta + z*del_mat
Y       <- linpred + rnorm(n, mean=0, sd=ep_sd)
data <- data.frame(y = Y, x_all_var, z, interaction_term)
data <- data %>% mutate_all(~(scale(.) %>% as.vector))
return(data)
}
data<-generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
duplicated(t(data))
duplicated(t(data) == ones(n))
duplicated(t(data) == rep(1, n))
data
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
print(duplicated(t(data)))
library(dplyr)
generate_data<-function(n, B, p, intercept, binom_prob, rnorm_mean, rnorm_sd, ep_sd, strong, zero, delta){
inter    <- intercept
strong_n <- p * strong #2
if (p %% 2 != 0){
norm_n  <- (p+1)/2 #3
binom_n <- p - norm_n #2
} else {
norm_n  <- p/2 #10
binom_n <- p/2 #10
}
# distribute binary/continuous for strong beta
if (strong_n %% 2 == 0){
strong_n_con  <- strong_n/2
strong_n_bin  <- strong_n/2
} else {
strong_n_con  <- (strong_n + 1)/2
strong_n_bin <- strong_n - strong_n_con
}
# distribute binary/continuous for zero beta
if (zero %% 2 == 0){
zero_n_con  <- zero/2
zero_n_bin  <- zero/2
} else {
zero_n_con  <- (zero_n + 1)/2
zero_n_bin <- zero_n - zero_n_con
}
# distribute binary/continuous
weak_n_con <- norm_n - strong_n_con - zero_n_con
weak_n_bin <- binom_n - strong_n_bin - zero_n_bin
if (weak_n_con <= 0){
weak_n_con = 0
}
if (weak_n_bin <= 0){
weak_n_bin = 0
}
# beta
binary_beta     <- c(rep(1, strong_n_bin), rep(0.5, weak_n_bin), rep(0, zero_n_bin))
con_beta        <- c(rep(1, strong_n_con), rep(0.5, weak_n_con), rep(0, zero_n_con))
true_beta       <- c(binary_beta, con_beta)
true_beta_table <- data.frame(variable = paste0("XZ", 1:p), true_beta)
# y prediction with interaction
x_all_var <- matrix(0, nrow=n, ncol=p)
z         <- rbinom(n, size=1, 0.5) # treatment indicator
for (j in 1:binom_n) {
x_all_var[, j] <- rbinom(n, size = 1, prob = binom_prob)
}
for (k in (norm_n+1):p) {
x_all_var[, k] <- rnorm(n, mean = rnorm_mean, sd = rnorm_sd)
}
gamma <- rep(1,p)
del_mat <- rep(delta,p)
interaction_term <- z*x_all_var
colnames(interaction_term) <- paste0('XZ',1:p)
linpred <- inter + x_all_var%*%gamma + interaction_term %*% true_beta + z*del_mat
Y       <- linpred + rnorm(n, mean=0, sd=ep_sd)
data <- data.frame(y = Y, x_all_var, z, interaction_term)
print(duplicated(t(data)))
data <- data %>% mutate_all(~(scale(.) %>% as.vector))
return(data)
}
data<-generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
library(dplyr)
generate_data<-function(n, B, p, intercept, binom_prob, rnorm_mean, rnorm_sd, ep_sd, strong, zero, delta){
inter    <- intercept
strong_n <- p * strong #2
if (p %% 2 != 0){
norm_n  <- (p+1)/2 #3
binom_n <- p - norm_n #2
} else {
norm_n  <- p/2 #10
binom_n <- p/2 #10
}
# distribute binary/continuous for strong beta
if (strong_n %% 2 == 0){
strong_n_con  <- strong_n/2
strong_n_bin  <- strong_n/2
} else {
strong_n_con  <- (strong_n + 1)/2
strong_n_bin <- strong_n - strong_n_con
}
# distribute binary/continuous for zero beta
if (zero %% 2 == 0){
zero_n_con  <- zero/2
zero_n_bin  <- zero/2
} else {
zero_n_con  <- (zero_n + 1)/2
zero_n_bin <- zero_n - zero_n_con
}
# distribute binary/continuous
weak_n_con <- norm_n - strong_n_con - zero_n_con
weak_n_bin <- binom_n - strong_n_bin - zero_n_bin
if (weak_n_con <= 0){
weak_n_con = 0
}
if (weak_n_bin <= 0){
weak_n_bin = 0
}
# beta
binary_beta     <- c(rep(1, strong_n_bin), rep(0.5, weak_n_bin), rep(0, zero_n_bin))
con_beta        <- c(rep(1, strong_n_con), rep(0.5, weak_n_con), rep(0, zero_n_con))
true_beta       <- c(binary_beta, con_beta)
true_beta_table <- data.frame(variable = paste0("XZ", 1:p), true_beta)
# y prediction with interaction
x_all_var <- matrix(0, nrow=n, ncol=p)
z         <- rbinom(n, size=1, 0.5) # treatment indicator
for (j in 1:binom_n) {
x_all_var[, j] <- rbinom(n, size = 1, prob = binom_prob)
}
for (k in (norm_n+1):p) {
x_all_var[, k] <- rnorm(n, mean = rnorm_mean, sd = rnorm_sd)
}
gamma <- rep(1,p)
del_mat <- rep(delta,p)
interaction_term <- z*x_all_var
colnames(interaction_term) <- paste0('XZ',1:p)
linpred <- inter + x_all_var%*%gamma + interaction_term %*% true_beta + z*del_mat
Y       <- linpred + rnorm(n, mean=0, sd=ep_sd)
data <- data.frame(y = Y, x_all_var, z, interaction_term)
print(any(duplicated(t(data))))
data <- data %>% mutate_all(~(scale(.) %>% as.vector))
return(data)
}
data<-generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
library(dplyr)
generate_data<-function(n, B, p, intercept, binom_prob, rnorm_mean, rnorm_sd, ep_sd, strong, zero, delta){
inter    <- intercept
strong_n <- p * strong #2
if (p %% 2 != 0){
norm_n  <- (p+1)/2 #3
binom_n <- p - norm_n #2
} else {
norm_n  <- p/2 #10
binom_n <- p/2 #10
}
# distribute binary/continuous for strong beta
if (strong_n %% 2 == 0){
strong_n_con  <- strong_n/2
strong_n_bin  <- strong_n/2
} else {
strong_n_con  <- (strong_n + 1)/2
strong_n_bin <- strong_n - strong_n_con
}
# distribute binary/continuous for zero beta
if (zero %% 2 == 0){
zero_n_con  <- zero/2
zero_n_bin  <- zero/2
} else {
zero_n_con  <- (zero_n + 1)/2
zero_n_bin <- zero_n - zero_n_con
}
# distribute binary/continuous
weak_n_con <- norm_n - strong_n_con - zero_n_con
weak_n_bin <- binom_n - strong_n_bin - zero_n_bin
if (weak_n_con <= 0){
weak_n_con = 0
}
if (weak_n_bin <= 0){
weak_n_bin = 0
}
# beta
binary_beta     <- c(rep(1, strong_n_bin), rep(0.5, weak_n_bin), rep(0, zero_n_bin))
con_beta        <- c(rep(1, strong_n_con), rep(0.5, weak_n_con), rep(0, zero_n_con))
true_beta       <- c(binary_beta, con_beta)
true_beta_table <- data.frame(variable = paste0("XZ", 1:p), true_beta)
# y prediction with interaction
x_all_var <- matrix(0, nrow=n, ncol=p)
z         <- rbinom(n, size=1, 0.5) # treatment indicator
for (j in 1:binom_n) {
x_all_var[, j] <- rbinom(n, size = 1, prob = binom_prob)
}
for (k in (norm_n+1):p) {
x_all_var[, k] <- rnorm(n, mean = rnorm_mean, sd = rnorm_sd)
}
gamma <- rep(1,p)
del_mat <- rep(delta,p)
interaction_term <- z*x_all_var
colnames(interaction_term) <- paste0('XZ',1:p)
linpred <- inter + x_all_var%*%gamma + interaction_term %*% true_beta + z*del_mat
Y       <- linpred + rnorm(n, mean=0, sd=ep_sd)
data <- data.frame(y = Y, x_all_var, z, interaction_term)
print(any(duplicated(t(data))))
data <- data %>% mutate_all(~(scale(.) %>% as.vector))
return(data)
}
data<-generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
library(dplyr)
generate_data<-function(n, B, p, intercept, binom_prob, rnorm_mean, rnorm_sd, ep_sd, strong, zero, delta){
inter    <- intercept
strong_n <- p * strong #2
if (p %% 2 != 0){
norm_n  <- (p+1)/2 #3
binom_n <- p - norm_n #2
} else {
norm_n  <- p/2 #10
binom_n <- p/2 #10
}
# distribute binary/continuous for strong beta
if (strong_n %% 2 == 0){
strong_n_con  <- strong_n/2
strong_n_bin  <- strong_n/2
} else {
strong_n_con  <- (strong_n + 1)/2
strong_n_bin <- strong_n - strong_n_con
}
# distribute binary/continuous for zero beta
if (zero %% 2 == 0){
zero_n_con  <- zero/2
zero_n_bin  <- zero/2
} else {
zero_n_con  <- (zero_n + 1)/2
zero_n_bin <- zero_n - zero_n_con
}
# distribute binary/continuous
weak_n_con <- norm_n - strong_n_con - zero_n_con
weak_n_bin <- binom_n - strong_n_bin - zero_n_bin
if (weak_n_con <= 0){
weak_n_con = 0
}
if (weak_n_bin <= 0){
weak_n_bin = 0
}
# beta
binary_beta     <- c(rep(1, strong_n_bin), rep(0.5, weak_n_bin), rep(0, zero_n_bin))
con_beta        <- c(rep(1, strong_n_con), rep(0.5, weak_n_con), rep(0, zero_n_con))
true_beta       <- c(binary_beta, con_beta)
true_beta_table <- data.frame(variable = paste0("XZ", 1:p), true_beta)
# y prediction with interaction
x_all_var <- matrix(0, nrow=n, ncol=p)
z         <- rbinom(n, size=1, 0.5) # treatment indicator
for (j in 1:binom_n) {
x_all_var[, j] <- rbinom(n, size = 1, prob = binom_prob)
}
for (k in (norm_n+1):p) {
x_all_var[, k] <- rnorm(n, mean = rnorm_mean, sd = rnorm_sd)
}
gamma <- rep(1,p)
del_mat <- rep(delta,p)
interaction_term <- z*x_all_var
colnames(interaction_term) <- paste0('XZ',1:p)
linpred <- inter + x_all_var%*%gamma + interaction_term %*% true_beta + z*del_mat
Y       <- linpred + rnorm(n, mean=0, sd=ep_sd)
data <- data.frame(y = Y, x_all_var, z, interaction_term)
print(any(duplicated(t(data))))
data <- data %>% mutate_all(~(scale(.) %>% as.vector))
return(data)
}
data<-generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
kappa(data)
res = NULL
#traceback()
for (i in 1:50){
data <- generate_data(n=100, B=100, p=20, intercept=0, binom_prob=0.1, rnorm_mean=0, rnorm_sd=0.1, ep_sd=1, strong=0.2, zero=16, delta=3)
#n=100; B=100; p=20; intercept=0; binom_prob=0.1; rnorm_mean=0; rnorm_sd=0.1; ep_sd=1; strong=0.2; zero=16; delta=3
x = glm(y~., data= data)
}
?vb
python_dat = py_load_object('data.pkl')
library(dplyr)
library(rstan)
library(jsonlite)
library(reticulate)
python_dat = py_load_object('data.pkl')
p = length(python_dat$alpha)
q = dim(python_dat$X)[2]
y = python_dat$y
X = python_dat$X
T = length(y)
data = list(T=T, p=p, q=q, y=y, X=X, power=1.)
model = stan_model(file = 'model_binaryAR.stan')
?vb
?vb
# Now we do VB
fit_vb = vb(model, data)
fit_vb$`alpha[1]`
fit_vb@sim$samples
extract(fit_vb)
samples = extract(fit_vb)
samples_vb = extract(fit_vb)
samples_vb$alpha
python_dat = py_load_object('data.pkl')
p = length(python_dat$alpha)
q = dim(python_dat$X)[2]
y = python_dat$y
X = python_dat$X
T = length(y)
data = list(T=T, p=p, q=q, y=y, X=X, power=1.)
model = stan_model(file = 'model_binaryAR.stan')
python_dat = py_load_object('data.pkl')
p = length(python_dat$alpha)
q = dim(python_dat$X)[2]
y = python_dat$y
X = python_dat$X
T = length(y)
data = list(T=T, p=p, q=q, y=y, X=X, power=1.)
model = stan_model(file = 'model_binaryAR.stan')
samples = extract(fit_vb)
write_json(laplace, "vb.json")
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
setwd("~/DC-BATS-deborshee/GARCH")
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
setwd("~/DC-BATS-deborshee/GARCH")
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
?setwd
getwd()
setwd("~/DC-BATS-deborshee/binary_AR")
setwd("~/DC-BATS-deborshee/GARCH")
library(dplyr)
library(rstan)
library(jsonlite)
library(reticulate)
python_dat = py_load_object('data.pkl')
p = length(python_dat$beta)
q = length(python_dat$alpha)
d = dim(python_dat$X)[1]
r = max(p, q)
y = python_dat$y
X = python_dat$X
T = length(y)
data = list(T=T, p=p, q=q, r=max(p,q), d=d, y=y, X=X, power=1.)
#data = dict(T=T, p=p, q=q, r=max(p,q), d=d, y=y, X=X, power=1.)
model = stan_model(file = 'garch_WB.stan')
fit = optimizing(model, data = data, hessian = TRUE, verbose = T)
laplace = list(mode = fit$par, hessian = fit$hessian, var_names = names(fit$par))
write_json(laplace, "laplace.json")
laplace$hessian
fit_vb = vb(model, data)
fit_vb = vb(model, data)
?vb
fit_vb = vb(model, data, algorithm="fullrank")
fit$par
fit_vb = vb(model, data, init=fit$par)
fit_vb = vb(model, data)
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
library(dplyr)
library(rstan)
library(jsonlite)
library(reticulate)
python_dat = py_load_object('data.pkl')
p = length(python_dat$alpha)
q = dim(python_dat$X)[2]
y = python_dat$y
X = python_dat$X
T = length(y)
data = list(T=T, p=p, q=q, y=y, X=X, power=1.)
model = stan_model(file = 'model_binaryAR.stan')
fit = optimizing(model, data = data, hessian = TRUE, verbose = T)
laplace = list(mode = fit$par, hessian = fit$hessian, var_names = names(fit$par))
write_json(laplace, "laplace.json")
laplace$hessian
# Now we do VB
fit_vb = vb(model, data)
vb_samples = extract(fit_vb)
write_json(vb_samples, "vb.json")
